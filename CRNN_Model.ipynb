{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1V-sWox0KWvxft0Xghhr4hHH9hiLH47EB","authorship_tag":"ABX9TyPZfSWcLcRlHSqioHXvKH86"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["DataLoader for Prescription Word Dataset"],"metadata":{"id":"y_R_OF_iPbi0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdIDwPjJLwYw"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image, ImageOps\n","\n","# Resize and pad images to (128, 32) - Same as old CRNN input size\n","def resize_and_pad(img, size=(128, 32)):\n","    img = img.convert(\"L\")  # Convert to grayscale\n","    w, h = img.size\n","    scale = size[1] / h\n","    new_w = int(w * scale)\n","    img = img.resize((new_w, size[1]), Image.BILINEAR)\n","\n","    if new_w > size[0]:\n","        img = img.resize(size, Image.BILINEAR)\n","    else:\n","        delta_w = size[0] - new_w\n","        padding = (delta_w // 2, 0, delta_w - delta_w // 2, 0)\n","        img = ImageOps.expand(img, padding, fill=255)\n","\n","    return img\n","\n","# Custom Dataset\n","class PrescriptionWordsDataset(Dataset):\n","    def __init__(self, csv_path, img_folder, img_size=(128, 32)):\n","        self.data = pd.read_csv(csv_path)\n","        self.img_folder = img_folder\n","        self.img_size = img_size\n","        self.transform = transforms.Compose([\n","            transforms.Lambda(lambda img: resize_and_pad(img, size=self.img_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5,), (0.5,))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name, label = self.data.iloc[idx]\n","        img_path = os.path.join(self.img_folder, img_name)\n","\n","        image = Image.open(img_path).convert(\"RGB\")\n","        image = self.transform(image)\n","\n","        return image, label\n"]},{"cell_type":"markdown","source":["Label Encoder for Characters"],"metadata":{"id":"gG-DlT9NPlHE"}},{"cell_type":"code","source":["# Label Encoder\n","class LabelEncoder:\n","    def __init__(self):\n","        self.characters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-.,:/()\"\n","        self.char2idx = {char: idx + 1 for idx, char in enumerate(self.characters)}  # 0 reserved for blank\n","        self.idx2char = {idx + 1: char for idx, char in enumerate(self.characters)}\n","        self.blank_label = 0\n","\n","    def encode(self, text):\n","        return [self.char2idx[char] for char in text if char in self.char2idx]\n","\n","    def decode(self, indices):\n","        return ''.join([self.idx2char.get(idx, '') for idx in indices if idx != self.blank_label])\n","\n","    def get_vocab_size(self):\n","        return len(self.char2idx) + 1\n"],"metadata":{"id":"y17d1ieQPpCt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build CRNN Model"],"metadata":{"id":"Xfyz85qEPs3K"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class CRNN(nn.Module):\n","    def __init__(self, img_height, num_classes):\n","        super(CRNN, self).__init__()\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(1, 64, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            nn.Conv2d(64, 128, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","\n","            nn.Conv2d(128, 256, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, 3, 1, 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","\n","            nn.Conv2d(256, 512, 3, 1, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, 3, 1, 1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d((2, 1)),\n","\n","            nn.Conv2d(512, 512, 2, 1, 0),\n","            nn.ReLU()\n","        )\n","\n","        self.rnn1 = nn.LSTM(512, 256, bidirectional=True, batch_first=True)\n","        self.rnn2 = nn.LSTM(512, 256, bidirectional=True, batch_first=True)\n","\n","\n","\n","        self.fc = nn.Linear(512, num_classes)\n","\n","    def forward(self, x):\n","      x = self.cnn(x)\n","      b, c, h, w = x.size()\n","      assert h == 1, \"Height must be 1 after convs\"\n","      x = x.squeeze(2)  # Remove height dimension\n","      x = x.permute(0, 2, 1)  # [batch, width, channels]\n","\n","      x, _ = self.rnn1(x)\n","      x, _ = self.rnn2(x)\n","      x = self.fc(x)\n","      return x\n"],"metadata":{"id":"NC7ToSKyPvW6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","from torch.nn import CTCLoss\n","\n","# Paths\n","csv_path = \"/content/drive/MyDrive/Individual_Research_Project/prescription_words_dataset.csv\"\n","img_folder = \"/content/drive/MyDrive/Individual_Research_Project/cropped_prescription_words\"\n","\n","# Parameters\n","batch_size = 32\n","epochs = 20\n","learning_rate = 0.001\n","img_height = 32\n","\n","# Device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Load dataset\n","train_dataset = PrescriptionWordsDataset(csv_path, img_folder)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Label encoder\n","encoder = LabelEncoder()\n","\n","# Model\n","vocab_size = encoder.get_vocab_size()\n","model = CRNN(img_height=img_height, num_classes=vocab_size).to(device)\n","\n","# Loss and Optimizer\n","criterion = CTCLoss(blank=0, zero_infinity=True)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eD4pYSCgPz2G","executionInfo":{"status":"ok","timestamp":1745852487864,"user_tz":-330,"elapsed":1969,"user":{"displayName":"Viduni Ukwatta","userId":"14455892817705871690"}},"outputId":"4cb27924-9a57-4a50-cfa8-6b6a701c2482"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["Model Training"],"metadata":{"id":"2GmD7e3vRz8-"}},{"cell_type":"code","source":["# Training Loop\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        # Encode labels\n","        targets = [encoder.encode(label) for label in labels]\n","        target_lengths = torch.IntTensor([len(t) for t in targets])\n","        targets = torch.IntTensor([i for seq in targets for i in seq])\n","\n","        images = images.to(device)\n","        targets = targets.to(device)\n","        target_lengths = target_lengths.to(device)\n","\n","        # Forward\n","        outputs = model(images)\n","        input_lengths = torch.full(size=(outputs.size(0),), fill_value=outputs.size(1), dtype=torch.long).to(device)\n","\n","        loss = criterion(outputs.log_softmax(2).transpose(0, 1), targets, input_lengths, target_lengths)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n"],"metadata":{"id":"1EQlUj4pR1qd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745856441806,"user_tz":-330,"elapsed":3950260,"user":{"displayName":"Viduni Ukwatta","userId":"14455892817705871690"}},"outputId":"0999dc79-8b0a-4bbb-dd04-d19c2fb60052"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20 | Loss: 3.9680\n","Epoch 2/20 | Loss: 2.9875\n","Epoch 3/20 | Loss: 2.7479\n","Epoch 4/20 | Loss: 2.5794\n","Epoch 5/20 | Loss: 2.4441\n","Epoch 6/20 | Loss: 2.3538\n","Epoch 7/20 | Loss: 2.2453\n","Epoch 8/20 | Loss: 2.1529\n","Epoch 9/20 | Loss: 2.0479\n","Epoch 10/20 | Loss: 1.9228\n","Epoch 11/20 | Loss: 1.8115\n","Epoch 12/20 | Loss: 1.7096\n","Epoch 13/20 | Loss: 1.5843\n","Epoch 14/20 | Loss: 1.5002\n","Epoch 15/20 | Loss: 1.3817\n","Epoch 16/20 | Loss: 1.3185\n","Epoch 17/20 | Loss: 1.2008\n","Epoch 18/20 | Loss: 1.1320\n","Epoch 19/20 | Loss: 1.0544\n","Epoch 20/20 | Loss: 0.9860\n"]}]},{"cell_type":"code","source":["os.makedirs(\"/content/drive/MyDrive/Individual_Research_Project/models/\", exist_ok=True)\n","torch.save(model.state_dict(), \"/content/drive/MyDrive/Individual_Research_Project/models/crnn_prescription_final.pth\")\n","print(\"Model saved successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eA1Yt8uO0MLM","executionInfo":{"status":"ok","timestamp":1745857067520,"user_tz":-330,"elapsed":101,"user":{"displayName":"Viduni Ukwatta","userId":"14455892817705871690"}},"outputId":"bb21ee43-a3a5-4b1f-c570-eb67a64508ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved successfully!\n"]}]}]}